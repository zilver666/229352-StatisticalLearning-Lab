{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zilver666/229352-StatisticalLearning-Lab/blob/main/Lab01_Data_Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X33DsAOeokmy"
      },
      "source": [
        "### Statistical Learning for Data Science 2 (229352)\n",
        "#### Instructor: Donlapark Ponnoprat\n",
        "\n",
        "#### [Course website](https://donlapark.pages.dev/229352/)\n",
        "\n",
        "## Lab #1\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, roc_curve\n",
        "\n",
        "# For Fashion-MNIST\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "\n",
        "# For 20 Newsgroups\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', 1000)"
      ],
      "metadata": {
        "id": "JOnoRSjo8dd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Marketing Campaign Dataset - Manual Data Preprocessing & Logistic Regression"
      ],
      "metadata": {
        "id": "exuopfRD8mHt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the Marketing Campaign Dataset ([Data Information](https://archive.ics.uci.edu/dataset/222/bank+marketing))\n",
        "\n",
        "The data is related with direct marketing campaigns of a Portuguese banking institution. The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be (`'yes'`) or not (`'no'`) subscribed."
      ],
      "metadata": {
        "id": "FdWsJYcVBf1Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bank_url = 'https://raw.githubusercontent.com/donlap/ds352-labs/main/bank.csv'\n",
        "\n",
        "df = pd.read_csv(bank_url, sep=';', na_values=['unknown'])\n",
        "df = df.drop([\"emp.var.rate\", \"cons.price.idx\", \"cons.conf.idx\",\t\"euribor3m\", \"nr.employed\"], axis=1)\n",
        "print(\"Shape of the dataset:\", df.shape)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "CJ9ujPUBBcTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Exploration"
      ],
      "metadata": {
        "id": "x4invEyyBRDI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Missing Values Count ---\")\n",
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "id": "u6wANpgzBQoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Unique Values for Categorical Columns ---\")\n",
        "for col in df.select_dtypes(include='object').columns:\n",
        "    print(f\"\\n'{col}' unique values:\")\n",
        "    print(df[col].value_counts(dropna=False)) # Include NaN counts"
      ],
      "metadata": {
        "id": "1ack00iLnuCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preprocessing"
      ],
      "metadata": {
        "id": "iCEtO_EWDTzC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Map target variable 'y' to 0 (no) and 1 (yes)\n",
        "df['y'] = # Write your code here\n",
        "\n",
        "\n",
        "# Drop 'duration' due to data leakage\n",
        "\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "\n",
        "\n",
        "# Split the data BEFORE any transformations\n",
        "\n",
        "\n",
        "# Print data shape\n",
        "\n"
      ],
      "metadata": {
        "id": "W2J1Prc_BXe7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will apply `StandardScaler()`, `OrdinalEncoder()`, and `OneHotEncoder()` on a few selected columns."
      ],
      "metadata": {
        "id": "yM_gRkNjGR53"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Numerical Feature: `age` and `campaign` (Standard Scaling)**"
      ],
      "metadata": {
        "id": "r4sPr5qDGY65"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_cols_demo = ['age', 'campaign']\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler ONLY on the training data\n",
        "\n"
      ],
      "metadata": {
        "id": "52ibTHHFDxSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Ordinal Feature: `education` (Ordinal Encoding with Imputation)**"
      ],
      "metadata": {
        "id": "8jQi5cJAGj_m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Imputation**"
      ],
      "metadata": {
        "id": "2uyk1rA7K7_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ord_col_demo = ['education']\n",
        "\n",
        "imputer_ord = SimpleImputer(strategy='most_frequent')\n",
        "\n",
        "## Write your code here\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "A8RbFYHLIhju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Ordinal Encoding**"
      ],
      "metadata": {
        "id": "P9hDuDPyLGaA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "education_categories = [\n",
        "    'illiterate', 'basic.4y', 'basic.6y', 'basic.9y', 'high.school',\n",
        "    'professional.course', 'university.degree', 'masters', 'doctorate'\n",
        "]"
      ],
      "metadata": {
        "id": "n7DMoQ6AGf8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ordinal_encoder = OrdinalEncoder(categories=[education_categories])\n",
        "\n",
        "## Write your code here\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MjbM7wi8IfHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Nominal Feature: `job` (One-Hot Encoding with Imputation)**"
      ],
      "metadata": {
        "id": "GarEnu8iK1lX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Imputation**"
      ],
      "metadata": {
        "id": "1eJSiEslLNi4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nom_col_demo = ['job']\n",
        "\n",
        "imputer_nom = SimpleImputer(strategy='most_frequent')\n",
        "imputer_nom.fit(X_train[nom_col_demo])\n",
        "\n",
        "X_train_imputed_nom_demo = imputer_nom.transform(X_train[nom_col_demo])\n",
        "X_test_imputed_nom_demo = imputer_nom.transform(X_test[nom_col_demo])"
      ],
      "metadata": {
        "id": "1OUbef1bJPUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Nominal Encoding**"
      ],
      "metadata": {
        "id": "l7F_kim6TQ72"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "onehot_encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        "\n",
        "## Write your code here\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6kMCGjksLyrf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 1: Apply All Preprocessing & Train Logistic Regression**\n",
        "\n",
        "Now, it's your turn to apply these preprocessing steps to *all* relevant columns and then train a Logistic Regression model.\n",
        "\n",
        "**Instructions:**\n",
        "\n",
        "1.  Look at the Variable Table in [this link](https://archive.ics.uci.edu/dataset/222/bank+marketing).\n",
        "2. Make lists for `numerical_features`, `ordinal_features`, and `nominal_features`.\n",
        "3. Preprocess the features. It is safer to make a copy of `X_train` using:\n",
        "   ```\n",
        "   X_train_copy = X_train.copy()\n",
        "   X_test_copy = X_test.copy()\n",
        "   ```\n",
        "   and preprocess `X_train_copy` instead.\n",
        "\n",
        "   **For nominal features, concat the one-hot encoded features using [`pd.concat(..., axis=1)`](https://pandas.pydata.org/docs/reference/api/pandas.concat.html) and drop the old nominal features from the dataframe.**\n",
        "4. Train Logistic Regression on the preprocessed `X_train_copy` and `y_train`.\n",
        "5. Evaluate the Model:\n",
        "    *   Make predictions on the preprocessed `X_test_copy`.\n",
        "    *   Print `classification_report` ([Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html)). What are the accuracy, average precision, average recall, and average f1-score?\n"
      ],
      "metadata": {
        "id": "CllpFvNBNYWI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- YOUR CODE FOR EXERCISE 1 STARTS HERE ---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xuI8hAlIRfDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Fashion-MNIST Dataset - Image Classification"
      ],
      "metadata": {
        "id": "m9qrm2DKRtgm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Fashion-MNIST Dataset\n",
        "\n",
        "The Fashion-MNIST dataset consists of 28x28 grayscale images of fashion items."
      ],
      "metadata": {
        "id": "kc8SZBvcS8_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(fm_X_train, fm_y_train), (fm_X_test, fm_y_test) = fashion_mnist.load_data()\n",
        "\n",
        "print(f\"Fashion-MNIST Train data shape: {fm_X_train.shape}\")\n",
        "print(f\"Fashion-MNIST Train labels shape: {fm_y_train.shape}\")\n",
        "print(f\"Fashion-MNIST Test data shape: {fm_X_test.shape}\")\n",
        "print(f\"Fashion-MNIST Test labels shape: {fm_y_test.shape}\")"
      ],
      "metadata": {
        "id": "r0FQt8rlRgoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"First image {fm_X_train[0]}\")\n",
        "print(f\"First label {fm_y_train[0]}\")"
      ],
      "metadata": {
        "id": "LXJ9EmcIVVrh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize Fashion-MNIST Images\n",
        "\n",
        "Let's see what these images look like."
      ],
      "metadata": {
        "id": "WwkQOE79TV70"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fashion_mnist_class_names = [\n",
        "    'T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "    'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'\n",
        "]\n",
        "\n",
        "# Visualize the images\n",
        "## Write your code here\n",
        "\n"
      ],
      "metadata": {
        "id": "-YUI6IzbTGYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 2: Preprocessing Images (Flatten and Scale)**\n",
        "\n",
        "Images are 2D arrays (matrices of pixels) and pixel values are integers from 0-255. For Logistic Regression, we need:\n",
        "*  **Flattening:** Convert each 28x28 image into a 1D array of 784 features.\n",
        "*  **Scaling:** Normalize pixel values from [0, 255] to [0, 1].\n",
        "\n",
        "**Instructions:**\n",
        "\n",
        "1.   **Flatten:** Use the `.reshape()` method (see [documentation](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.reshape.html)). For `fm_X_train_binary` (shape `(num_samples, 28, 28)`), you want to reshape it to `(num_samples, 28*28)`.\n",
        "2.  **Scale:** Divide the flattened pixel values by 255.0 to get values between 0 and 1.\n",
        "3.   **Train Logistic Regression:**\n",
        "    *   Initialize `LogisticRegression(solver='saga')`. `saga` is a good solver when both number of samples and number of features are large.\n",
        "    *   Fit the model on your *processed* `fm_X_train_scaled` and `fm_y_train`.\n",
        "4.   **Make Predictions:** Use `predict()` to make predictions on the *processed* `fm_X_test_scaled`.\n",
        "5.   **Print Classification Report:** Print `classification_report` ([Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html)). What are the accuracy, average precision, average recall, and average f1-score?\n",
        "6.   **Visualize Misclassifications:**\n",
        "    *   Find the indices in `fm_X_test_binary` where your model made incorrect predictions (i.e., `fm_y_pred != fm_y_test`).\n",
        "    *   Select 5 of these misclassified images.\n",
        "    *   Plot these images (using `plt.imshow`). For each image, print its true label and its predicted label."
      ],
      "metadata": {
        "id": "cXvJB42xVrHu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- YOUR CODE FOR EXERCISE 2 STARTS HERE ---\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "P_Z7ooAgdLVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: 20 Newsgroups Dataset - Text Classification"
      ],
      "metadata": {
        "id": "u7-q5FmnboVJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load 20 Newsgroups Dataset\n",
        "\n",
        "The 20 newsgroups dataset comprises around 18000 newsgroups posts on 20 topics."
      ],
      "metadata": {
        "id": "6TDncAyyb6mJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "news_train = fetch_20newsgroups(subset='train', shuffle=True, random_state=42)\n",
        "news_test = fetch_20newsgroups(subset='test', shuffle=True, random_state=42)\n",
        "\n",
        "X_train_news, y_train_news = news_train.data, news_train.target\n",
        "X_test_news, y_test_news = news_test.data, news_test.target\n",
        "\n",
        "print(f\"Number of training documents: {len(X_train_news)}\")\n",
        "print(f\"Number of test documents: {len(X_test_news)}\")\n",
        "print(f\"Categories: {news_train.target_names}\")"
      ],
      "metadata": {
        "id": "E91xZl5NbnpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explore Sample Document"
      ],
      "metadata": {
        "id": "tVCq09V2cfGj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the first document and its class\n",
        "## Write your code here\n",
        "\n"
      ],
      "metadata": {
        "id": "G37RNZGebFZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing: Text Vectorization Demonstration with `TfidfVectorizer`"
      ],
      "metadata": {
        "id": "1vXXUdp7chsX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n",
        "\\text{TF-IDF}(t, d, D) = \\text{TF}(t, d) \\times \\text{IDF}(t, D)\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "$$\n",
        "\\text{TF}(t, d) = \\frac{\\text{number of word }t\\text{ in } d}{\\text{number of words in } d} \\quad \\text{ and } \\quad\n",
        "\\text{IDF}(t, D) = \\log\\left(\\frac{\\text{total number of documents}}{\\text{number of documents that contain word }t}\\right).\n",
        "$$"
      ],
      "metadata": {
        "id": "rwaq1igbi-Hp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_sentences = [\n",
        "    \"This is the first document.\",\n",
        "    \"This document is the second document.\",\n",
        "    \"And this is the third one.\",\n",
        "    \"Is this the first document?\"\n",
        "]\n",
        "\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "\n",
        "# Fit and transform the sample sentences\n",
        "sample_vec_output_sparse = # Write your code here\n",
        "\n",
        "sample_vec_output_dense = sample_vec_output_sparse.toarray()\n",
        "\n",
        "print(vectorizer.vocabulary_)\n",
        "print(vectorizer.get_feature_names_out())\n",
        "print(sample_vec_output_dense)"
      ],
      "metadata": {
        "id": "8AaVd2D9ckKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 3: Apply TF-IDF Vectorization to Full Dataset**\n",
        "\n",
        "Now, apply `TfidfVectorizer` to the actual training and testing datasets for the 20 Newsgroups classification task.\n",
        "\n",
        "**Instructions:**\n",
        "\n",
        "1.  **Initialize `TfidfVectorizer`:**\n",
        "    *   Initialize `TfidfVectorizer`. Use `stop_words='english'` to remove common words.\n",
        "2.  **Fit and Transform Training Data:**\n",
        "    *   Call `fit_transform()` on `X_train_news` to learn the vocabulary and transform the training text into TF-IDF features. Store the result in `X_train_vec`.\n",
        "3.  **Transform Test Data:**\n",
        "    *   Call `transform()` on `X_test_news` using the *already fitted* vectorizer. Store the result in `X_test_vec`. **Crucially, do not call `fit_transform()` on the test data!** This would cause data leakage.\n",
        "4.  **Initialize Logistic Regression:**\n",
        "    *   Initialize `LogisticRegression(solver='saga')`. `saga` is a good solver when both number of samples and number of features are large.\n",
        "5.  **Train the Model:**\n",
        "    *   Fit the model on your `X_train_vec` and `y_train_news`.\n",
        "6.  **Make Predictions:**\n",
        "    *   Make predictions using `predict()` on the `X_test_vec`.\n",
        "7.  **Evaluate the Model:**\n",
        "    *   Print `classification_report` ([Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html)). What are the accuracy, average precision, average recall, and average f1-score?"
      ],
      "metadata": {
        "id": "aCa_dcEDc-PQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- YOUR CODE FOR EXERCISE 3 STARTS HERE ---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "B6g7y5yXrczq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}